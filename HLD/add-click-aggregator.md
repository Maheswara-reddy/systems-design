## The need of an Ad Click Aggregator
An ad click aggregator is essential because writing billions of raw ad clicks directly to a database in real-time would instantly overwhelm the system and crash the user's redirect flow. By buffering these raw events through a message queue and grouping them into aggregated time buckets (e.g., "100 clicks on Ad A at 2:00 PM"), the aggregator drastically reduces database write loads and storage costs. Ultimately, it acts as the crucial processing layer that filters out bot traffic, drops duplicate clicks, and powers near-real-time analytics dashboards, ensuring advertisers are billed accurately without bringing your infrastructure down.

## 1. The Core Motivation

An ad click aggregator solves the fundamental conflict between **massive ingestion scale** and the need for **real-time billing and analytics**.

* **Decoupling:** It protects the database from being overwhelmed by using a message broker to absorb massive traffic spikes.
* **Data Compression:** It drastically reduces storage costs and improves query performance by grouping raw events into time buckets (e.g., 1-minute intervals).
* **Fraud Prevention:** It acts as a filter to ensure advertisers are only billed for legitimate clicks, utilizing deduplication and anomaly detection.

---

## 2. High-Level Architecture Pipeline

The system moves data from raw clicks to queryable dashboards through distinct layers:

* **Ingestion (API Servers):** Lightweight servers handle the incoming clicks. They do two things instantly: drop the raw event into a message broker and redirect the user.
* **Message Broker (Apache Kafka):** Acts as the system's shock absorber. Clicks are partitioned by `ad_id` to guarantee ordering and route all clicks for a specific ad to the same downstream consumer.
* **Stream Processor (Apache Flink/Spark):** The engine that reads the Kafka stream, aggregates the clicks into time buckets, deduplicates, and filters out bots.
* **Storage (Hot & Cold):** Segregates the workload. Hot storage handles real-time dashboard queries, while cold storage keeps raw logs for historical analysis.

---

## 3. Storage and Schema Design

We rely on different database paradigms to handle the unique read/write patterns of the system.

### The Redirection Cache (Key-Value + Relational)

Handling the "TinyURL" aspect of routing users to the advertiser's landing page requires extreme read speed.

* **Flow:** The API queries Redis for `ad_id -> target_url`.
* **Fallback:** On a cache miss, it queries a relational DB (like PostgreSQL), updates Redis, and redirects.

### Multi-Dimensional Aggregation (Hot OLTP Storage)

To power real-time dashboards that allow filtering without querying raw data, we use a Wide-Column NoSQL database (like Cassandra) or DynamoDB.

| Column | Key Type | Purpose |
| --- | --- | --- |
| `ad_id` | Partition Key | Distributes load across nodes. |
| `date_bucket` | Partition Key | Prevents hot partitions for viral ads. |
| `minute_timestamp` | Clustering Key | Sorts data chronologically on disk. |
| `country` / `device_type` | Clustering Key | Enables dashboard filtering (dimensions). |
| `click_count` | Value | The aggregated total. |

### The Data Lake (Cold OLAP Storage)

A separate Kafka consumer group reads the exact same raw events and dumps them into object storage (like AWS S3) in a columnar format (like Parquet). This powers deep historical queries using distributed engines like Snowflake.

---

## 4. Solving the Hard Distributed Systems Problems

This is where the architecture becomes robust and interview-ready.

### Fraud and Deduplication

* **Accidental Double Clicks (Fast Path):** Use a Redis cache with a short TTL (e.g., 60 seconds) keyed by `user_id:ad_id`. If the key exists, drop the event.
* **Malicious Bots (Stream Path):** Use sliding window aggregations in the stream processor keyed by `ip_address`. If an IP exceeds a threshold within a short window, flag it, drop the clicks, and route to a dead-letter queue.

### Exactly-Once Processing

To ensure an advertiser is never double-billed if a consumer crashes:

* Use Flink's **distributed checkpoints (two-phase commit)**, which saves the aggregation state and Kafka offset simultaneously.
* Alternatively, use **atomic database writes**, updating the `click_count` and the `kafka_offset` in the exact same database transaction.

### Real-Time Dashboard Delivery

* **Short Polling:** The frontend requests updates every 10-15 seconds from the fast NoSQL database.
* **WebSockets:** The stream processor pushes updates to a Redis Pub/Sub channel, which backend servers use to push live updates to the browser via WebSockets.

### Late-Arriving Events

Handling events delayed by network drops (e.g., losing signal in a tunnel):

* **Event Time vs. Processing Time:** Always aggregate based on the timestamp generated by the client device (Event Time), not when the server received it.
* **Watermarks:** Introduce a delay (e.g., 3 minutes). The system waits until the watermark passes a specific time bucket before closing it and writing it to the database.
* **Allowed Lateness:** For events that arrive *after* the watermark has passed, configure the stream processor to emit an "Upsert" command. Because the NoSQL database uses `ad_id` and `timestamp` as keys, it seamlessly overwrites the old aggregate with the newly incremented total.

---


## Cassandra is famous for high write speeds but how does it handle the reads?
- Can it handle the fast read speeds required by the dashboards?
    - The answer is **yes, but only if you perfectly predict every single way the dashboard will be used.** Here is exactly how Cassandra handles the dashboard reads, where it falls apart, and how senior engineers fix it.

### 1. Where Cassandra Excels: The Predictable Dashboard

If your dashboard opens and immediately loads a chart showing the clicks for a specific ad over the last 24 hours, Cassandra is incredibly fast.

* **The Query:** `SELECT * FROM ad_clicks WHERE ad_id = 123 AND date_bucket = '2026-02-22' AND minute_timestamp > '10:00:00'`
* **Why it's fast:** Because `ad_id` and `date_bucket` are your Partition Keys, Cassandra does an O(1) hash lookup, goes directly to a single server node, and sequentially reads the time-series data right off the disk. It will return this data in single-digit milliseconds.

### 2. Where Cassandra Fails: The "Slice and Dice" Dashboard

Dashboards usually have dropdown filters. What happens when an advertiser logs in and says, *"I want to see the total clicks across **all** my ads, but only for users in India on mobile devices"?*

If your query doesn't include the `ad_id` (the Partition Key), Cassandra panics.

* It doesn't know which node holds the data.
* It is forced into a **Scatter-Gather** anti-pattern, sending the query to every single node in the cluster.
* If your cluster has 50 nodes, the coordinator node has to wait for 50 responses, filter out the non-India/non-mobile clicks in memory, and merge the results.
* Read latency spikes from 5ms to 5 seconds (or the query simply times out). The dashboard crashes.

### 3. The Cassandra Solution: Denormalization

In the relational database world, you solve new query patterns by adding a Secondary Index. In the Cassandra world, storage is incredibly cheap, but CPU and network hops are expensive.

To solve the dashboard problem, you **write the same data multiple times into completely different tables** structured for specific reads.

When the Flink stream processor emits a 1-minute aggregated bucket, it doesn't write to one table; it writes to three:

1. **Table 1 (For the Ad View):** Partition Key: `ad_id`.
2. **Table 2 (For the Campaign View):** Partition Key: `campaign_id`.
3. **Table 3 (For the Geography View):** Partition Key: `country`.

Now, no matter which dropdown the advertiser clicks on the dashboard, the backend API queries a table that is specifically partitioned for that exact request, guaranteeing millisecond reads.

### 4. The Modern Senior Engineer Alternative: Real-Time OLAP

While Cassandra *can* power these dashboards via heavy denormalization, the modern industry standard for this specific problem (high-throughput ingestion + complex dashboard filtering) is to use a **Real-Time OLAP Database**.

Technologies like **Apache Druid**, **Apache Pinot**, or **ClickHouse** are purpose-built for exactly this.

* They can ingest millions of streaming events per second directly from Kafka.
* They automatically index *every single column*.
* They allow advertisers to dynamically slice, dice, and filter by `ad_id`, `country`, `device_type`, or `campaign_id` in milliseconds without you having to maintain 10 different denormalized Cassandra tables.

---

In an interview, starting with Cassandra for the hot store shows you understand high-scale writes. Pivoting to acknowledge its read limitations and suggesting data denormalization—or proposing Apache Druid for complex dashboards—proves you have Senior-level foresight.
