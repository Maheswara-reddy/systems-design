## The need of an Ad Click Aggregator
An ad click aggregator is essential because writing billions of raw ad clicks directly to a database in real-time would instantly overwhelm the system and crash the user's redirect flow. By buffering these raw events through a message queue and grouping them into aggregated time buckets (e.g., "100 clicks on Ad A at 2:00 PM"), the aggregator drastically reduces database write loads and storage costs. Ultimately, it acts as the crucial processing layer that filters out bot traffic, drops duplicate clicks, and powers near-real-time analytics dashboards, ensuring advertisers are billed accurately without bringing your infrastructure down.

## 1. The Core Motivation

An ad click aggregator solves the fundamental conflict between **massive ingestion scale** and the need for **real-time billing and analytics**.

* **Decoupling:** It protects the database from being overwhelmed by using a message broker to absorb massive traffic spikes.
* **Data Compression:** It drastically reduces storage costs and improves query performance by grouping raw events into time buckets (e.g., 1-minute intervals).
* **Fraud Prevention:** It acts as a filter to ensure advertisers are only billed for legitimate clicks, utilizing deduplication and anomaly detection.

---

## 2. High-Level Architecture Pipeline

The system moves data from raw clicks to queryable dashboards through distinct layers:

* **Ingestion (API Servers):** Lightweight servers handle the incoming clicks. They do two things instantly: drop the raw event into a message broker and redirect the user.
* **Message Broker (Apache Kafka):** Acts as the system's shock absorber. Clicks are partitioned by `ad_id` to guarantee ordering and route all clicks for a specific ad to the same downstream consumer.
* **Stream Processor (Apache Flink/Spark):** The engine that reads the Kafka stream, aggregates the clicks into time buckets, deduplicates, and filters out bots.
* **Storage (Hot & Cold):** Segregates the workload. Hot storage handles real-time dashboard queries, while cold storage keeps raw logs for historical analysis.

---

## 3. Storage and Schema Design

We rely on different database paradigms to handle the unique read/write patterns of the system.

### The Redirection Cache (Key-Value + Relational)

Handling the "TinyURL" aspect of routing users to the advertiser's landing page requires extreme read speed.

* **Flow:** The API queries Redis for `ad_id -> target_url`.
* **Fallback:** On a cache miss, it queries a relational DB (like PostgreSQL), updates Redis, and redirects.

### Multi-Dimensional Aggregation (Hot OLTP Storage)

To power real-time dashboards that allow filtering without querying raw data, we use a Wide-Column NoSQL database (like Cassandra) or DynamoDB.

| Column | Key Type | Purpose |
| --- | --- | --- |
| `ad_id` | Partition Key | Distributes load across nodes. |
| `date_bucket` | Partition Key | Prevents hot partitions for viral ads. |
| `minute_timestamp` | Clustering Key | Sorts data chronologically on disk. |
| `country` / `device_type` | Clustering Key | Enables dashboard filtering (dimensions). |
| `click_count` | Value | The aggregated total. |

### The Data Lake (Cold OLAP Storage)

A separate Kafka consumer group reads the exact same raw events and dumps them into object storage (like AWS S3) in a columnar format (like Parquet). This powers deep historical queries using distributed engines like Snowflake.

---

## 4. Solving the Hard Distributed Systems Problems

This is where the architecture becomes robust and interview-ready.

### Fraud and Deduplication

* **Accidental Double Clicks (Fast Path):** Use a Redis cache with a short TTL (e.g., 60 seconds) keyed by `user_id:ad_id`. If the key exists, drop the event.
* **Malicious Bots (Stream Path):** Use sliding window aggregations in the stream processor keyed by `ip_address`. If an IP exceeds a threshold within a short window, flag it, drop the clicks, and route to a dead-letter queue.

### Exactly-Once Processing

To ensure an advertiser is never double-billed if a consumer crashes:

* Use Flink's **distributed checkpoints (two-phase commit)**, which saves the aggregation state and Kafka offset simultaneously.
* Alternatively, use **atomic database writes**, updating the `click_count` and the `kafka_offset` in the exact same database transaction.

### Real-Time Dashboard Delivery

* **Short Polling:** The frontend requests updates every 10-15 seconds from the fast NoSQL database.
* **WebSockets:** The stream processor pushes updates to a Redis Pub/Sub channel, which backend servers use to push live updates to the browser via WebSockets.

### Late-Arriving Events

Handling events delayed by network drops (e.g., losing signal in a tunnel):

* **Event Time vs. Processing Time:** Always aggregate based on the timestamp generated by the client device (Event Time), not when the server received it.
* **Watermarks:** Introduce a delay (e.g., 3 minutes). The system waits until the watermark passes a specific time bucket before closing it and writing it to the database.
* **Allowed Lateness:** For events that arrive *after* the watermark has passed, configure the stream processor to emit an "Upsert" command. Because the NoSQL database uses `ad_id` and `timestamp` as keys, it seamlessly overwrites the old aggregate with the newly incremented total.

---
